{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription App Notebook\n",
    "\n",
    "This notebook demonstrates how to process a meeting video (or audio) file to:\n",
    "- Extract the audio from the video.\n",
    "- Transcribe the audio using an ASR model (e.g., Whisper).\n",
    "- Perform speaker detection and segmentation with Silero VAD.\n",
    "- Cluster the speech segments to assign speaker labels.\n",
    "- Optionally assign conversation roles (e.g., Interviewer/Interviewee) using simple heuristics.\n",
    "\n",
    "Below, you'll find code cells for each major step in the transcription process along with detailed explanations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role Assignment\n",
    "\n",
    "Using a simple heuristic (such as counting question marks), we assign conversation roles (e.g., \"Interviewer\" vs. \"Interviewee\") to each transcript segment. This step can be further improved using more advanced NLP methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_roles(segments):\n",
    "    \"\"\"\n",
    "    Assign roles to speakers using a simple heuristic.\n",
    "    In this example, the speaker with the highest number of question marks is designated as \"Interviewer\".\n",
    "    \"\"\"\n",
    "    speaker_texts = {}\n",
    "    for seg in segments:\n",
    "        speaker = seg.get('speaker', 'Unknown')\n",
    "        text = seg.get('text', '')\n",
    "        speaker_texts[speaker] = speaker_texts.get(speaker, \"\") + \" \" + text\n",
    "    question_counts = {speaker: text.count('?') for speaker, text in speaker_texts.items()}\n",
    "    interviewer = max(question_counts, key=question_counts.get) if question_counts else None\n",
    "    \n",
    "    for seg in segments:\n",
    "        speaker = seg.get('speaker', 'Unknown')\n",
    "        if speaker == interviewer and question_counts.get(interviewer, 0) > 0:\n",
    "            seg['role'] = \"Interviewer\"\n",
    "        else:\n",
    "            seg['role'] = \"Interviewee\"\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_formatted_transcript(segments, output_file):\n",
    "    \"\"\"\n",
    "    Save the transcript in the format: [role] speaker: text\n",
    "    Returns a list of transcript lines (each as a dictionary).\n",
    "    \"\"\"\n",
    "    transcript_lines = []\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            current_role = None\n",
    "            current_speaker = None\n",
    "            current_text = \"\"\n",
    "            for seg in segments:\n",
    "                role = seg.get('role', 'Unknown')\n",
    "                speaker = seg.get('speaker', 'Unknown')\n",
    "                text = seg.get('text', '').strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                if (role, speaker) != (current_role, current_speaker) and current_text:\n",
    "                    line = {\"role\": current_role, \"speaker\": current_speaker, \"text\": current_text}\n",
    "                    transcript_lines.append(line)\n",
    "                    f.write(f\"[{current_role}] {current_speaker}: {current_text}\\n\")\n",
    "                    current_text = \"\"\n",
    "                current_role = role\n",
    "                current_speaker = speaker\n",
    "                if current_text:\n",
    "                    current_text += \" \" + text\n",
    "                else:\n",
    "                    current_text = text\n",
    "            if current_text:\n",
    "                line = {\"role\": current_role, \"speaker\": current_speaker, \"text\": current_text}\n",
    "                transcript_lines.append(line)\n",
    "                f.write(f\"[{current_role}] {current_speaker}: {current_text}\\n\")\n",
    "        print(f\"Role-based transcript saved to {output_file}\")\n",
    "        return transcript_lines\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving transcript: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Detection and Clustering\n",
    "\n",
    "Here we load the Silero VAD model to detect speech segments within the audio. We then compute simple embeddings for each segment and use Agglomerative Clustering to differentiate between speakers. Each segment is assigned a speaker label (e.g., Speaker 1, Speaker 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_silero_vad():\n",
    "    \"\"\"\n",
    "    Load the Silero VAD model and helper functions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading Silero VAD model...\")\n",
    "        model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                      model='silero_vad',\n",
    "                                      force_reload=True,\n",
    "                                      onnx=False)\n",
    "        (get_speech_timestamps, _, read_audio, _, _) = utils\n",
    "        return model, get_speech_timestamps, read_audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Silero VAD: {e}\")\n",
    "        return None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_speech_embeddings(audio_path, vad_model, get_speech_timestamps, read_audio):\n",
    "    \"\"\"\n",
    "    Detect speech segments and compute simple embeddings.\n",
    "    Returns an array of embeddings and a list of segment dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wav = read_audio(audio_path, sampling_rate=16000)\n",
    "        speech_timestamps = get_speech_timestamps(wav, vad_model, sampling_rate=16000, \n",
    "                                                    min_speech_duration_ms=500, \n",
    "                                                    max_speech_duration_s=float('inf'),\n",
    "                                                    min_silence_duration_ms=500)\n",
    "        print(f\"Detected {len(speech_timestamps)} speech segments\")\n",
    "        embeddings = []\n",
    "        segments = []\n",
    "        \n",
    "        def get_speech_segment(start_sample, end_sample):\n",
    "            return wav[start_sample:end_sample]\n",
    "        \n",
    "        for i, segment in enumerate(speech_timestamps):\n",
    "            start_sample = segment['start']\n",
    "            end_sample = segment['end']\n",
    "            speech_segment = get_speech_segment(start_sample, end_sample)\n",
    "            if len(speech_segment) < 1600:\n",
    "                continue\n",
    "            # Use clone().detach() to avoid warnings.\n",
    "            segment_tensor = torch.tensor(speech_segment).clone().detach().float()\n",
    "            spec = torch.stft(\n",
    "                segment_tensor,\n",
    "                n_fft=512,\n",
    "                hop_length=160,\n",
    "                win_length=400,\n",
    "                window=torch.hann_window(400),\n",
    "                return_complex=True\n",
    "            )\n",
    "            spec = torch.abs(spec)\n",
    "            freq_bands = [(0, 10), (10, 20), (20, 50), (50, 100), (100, 256)]\n",
    "            feature_vector = []\n",
    "            for low, high in freq_bands:\n",
    "                band_energy = torch.mean(spec[low:high, :]).item()\n",
    "                feature_vector.append(band_energy)\n",
    "            zero_crossings = torch.sum(torch.abs(torch.sign(segment_tensor[1:]) - torch.sign(segment_tensor[:-1]))).item() / 2\n",
    "            zero_crossing_rate = zero_crossings / len(segment_tensor)\n",
    "            feature_vector.append(zero_crossing_rate)\n",
    "            energy = torch.mean(torch.abs(segment_tensor)).item()\n",
    "            feature_vector.append(energy)\n",
    "            start_time = start_sample / 16000\n",
    "            end_time = end_sample / 16000\n",
    "            embeddings.append(feature_vector)\n",
    "            segments.append({\n",
    "                'start': start_time,\n",
    "                'end': end_time,\n",
    "                'length': end_time - start_time\n",
    "            })\n",
    "        return np.array(embeddings), segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting speech embeddings: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_speakers(embeddings, segments, min_speakers=2, max_speakers=5):\n",
    "    \"\"\"\n",
    "    Cluster speech segments to assign speaker labels.\n",
    "    \"\"\"\n",
    "    if len(embeddings) == 0:\n",
    "        print(\"No speech segments detected\")\n",
    "        return []\n",
    "    \n",
    "    embeddings = (embeddings - np.mean(embeddings, axis=0)) / (np.std(embeddings, axis=0) + 1e-8)\n",
    "    best_score = -1\n",
    "    best_labels = None\n",
    "    best_n_clusters = min_speakers\n",
    "    max_speakers = min(max_speakers, len(embeddings))\n",
    "    \n",
    "    for n_clusters in range(min_speakers, max_speakers + 1):\n",
    "        if n_clusters >= len(embeddings):\n",
    "            continue\n",
    "        \n",
    "        # Remove affinity parameter when using ward linkage.\n",
    "        clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "        labels = clustering.fit_predict(embeddings)\n",
    "        if n_clusters == 1 or len(set(labels)) <= 1:\n",
    "            continue\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        print(f\"Clusters: {n_clusters}, Silhouette Score: {score:.3f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_labels = labels\n",
    "            best_n_clusters = n_clusters\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        if i < len(best_labels):\n",
    "            segment['speaker'] = f\"Speaker {best_labels[i] + 1}\"\n",
    "    print(f\"Selected {best_n_clusters} speakers with silhouette score: {best_score:.3f}\")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_transcript_to_speakers(whisper_segments, vad_segments):\n",
    "    \"\"\"\n",
    "    Align transcript segments from Whisper with VAD segments based on time overlap.\n",
    "    \"\"\"\n",
    "    for w_segment in whisper_segments:\n",
    "        w_start = w_segment['start']\n",
    "        w_end = w_segment['end']\n",
    "        best_overlap = 0\n",
    "        best_speaker = \"Unknown\"\n",
    "        for v_segment in vad_segments:\n",
    "            v_start = v_segment['start']\n",
    "            v_end = v_segment['end']\n",
    "            overlap_start = max(w_start, v_start)\n",
    "            overlap_end = min(w_end, v_end)\n",
    "            if overlap_end > overlap_start:\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                if overlap_duration > best_overlap:\n",
    "                    best_overlap = overlap_duration\n",
    "                    if 'speaker' in v_segment:\n",
    "                        best_speaker = v_segment['speaker']\n",
    "        w_segment['speaker'] = best_speaker\n",
    "    return whisper_segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Extraction from Video\n",
    "\n",
    "In this section, we extract the audio track from a given video file using the `pydub` library. The audio is converted to mono and resampled to 16kHz to ensure compatibility with the speech models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_audio(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio from a video file and save as a WAV file.\n",
    "    Converts to mono and 16kHz sample rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(video_path):\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "        video = AudioSegment.from_file(video_path)\n",
    "        video = video.set_channels(1)\n",
    "        video = video.set_frame_rate(16000)\n",
    "        video.export(audio_path, format=\"wav\")\n",
    "        print(f\"Audio extracted successfully to {audio_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting video to audio: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription with Whisper\n",
    "\n",
    "This section loads the Whisper ASR model and transcribes the extracted audio. The output includes the full transcript along with time-stamped segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, model_name=\"base\"):\n",
    "    \"\"\"\n",
    "    Transcribe the audio using Whisper.\n",
    "    Returns the transcription result dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading Whisper model: {model_name}\")\n",
    "        model = whisper.load_model(model_name)\n",
    "        print(\"Transcribing audio...\")\n",
    "        result = model.transcribe(audio_path, verbose=True)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing audio: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript_file(transcript_file):\n",
    "    \"\"\"\n",
    "    Parse the transcript file and create a list of dictionaries for insertion.\n",
    "    Each line in the transcript is assumed to be in the format:\n",
    "      [Role] Speaker Label: Transcript text\n",
    "    The function extracts:\n",
    "      - role: text within the square brackets\n",
    "      - speaker_label: text between the square bracket and the colon\n",
    "      - transcript: text after the colon.\n",
    "    Since no meeting_id, start_time, or end_time information is available, these will be set to None.\n",
    "    \"\"\"\n",
    "    transcript_lines = []\n",
    "    with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Find the role in square brackets\n",
    "            if line.startswith('['):\n",
    "                try:\n",
    "                    role_end = line.index(']')\n",
    "                    role = line[1:role_end]\n",
    "                except ValueError:\n",
    "                    role = \"Unknown\"\n",
    "            else:\n",
    "                role = \"Unknown\"\n",
    "            \n",
    "            # Split at the colon\n",
    "            if ':' in line:\n",
    "                header, transcript_text = line.split(':', 1)\n",
    "            else:\n",
    "                header, transcript_text = line, \"\"\n",
    "            \n",
    "            # Remove the role part from header to get the speaker label\n",
    "            speaker_label = header.replace(f'[{role}]', '').strip()\n",
    "            \n",
    "            transcript_lines.append({\n",
    "                \"meeting_id\": None,\n",
    "                \"speaker_label\": f\"{role} - {speaker_label}\",\n",
    "                \"transcript\": transcript_text.strip(),\n",
    "                \"start_time\": None,\n",
    "                \"end_time\": None\n",
    "            })\n",
    "    return transcript_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Float\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARSH DADIYA\\AppData\\Local\\Temp\\ipykernel_9912\\4175677954.py:2: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "DB_URL = os.getenv(\"DB_URL\", \"postgresql+psycopg2://your_db_user:your_db_password@localhost/your_db_name\")\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcript(Base):\n",
    "    __tablename__ = 'transcripts'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    meeting_id = Column(String(50), nullable=True)  # You can provide a meeting ID if available.\n",
    "    speaker_label = Column(String(100))\n",
    "    transcript = Column(Text)\n",
    "    start_time = Column(Float, nullable=True)  # e.g., seconds into the video.\n",
    "    end_time = Column(Float, nullable=True)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "\n",
    "def insert_transcript_lines_sqlalchemy(db_url = DB_URL, transcript_lines=[]):\n",
    "    \"\"\"\n",
    "    Insert transcript lines into the PostgreSQL database using SQLAlchemy.\n",
    "    \n",
    "    Each transcript_line in transcript_lines should be a dictionary with keys:\n",
    "      - meeting_id (optional)\n",
    "      - speaker_label (e.g., \"Interviewer - Speaker 1\")\n",
    "      - transcript (the text content)\n",
    "      - start_time (float)\n",
    "      - end_time (float)\n",
    "    \"\"\"\n",
    "    engine = create_engine(db_url)\n",
    "    Base.metadata.create_all(engine)  # Create table if it doesn't exist.\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    try:\n",
    "        for line in transcript_lines:\n",
    "            transcript_entry = Transcript(\n",
    "                meeting_id=line.get('meeting_id'),\n",
    "                speaker_label=line.get('speaker_label'),\n",
    "                transcript=line.get('transcript'),\n",
    "                start_time=line.get('start_time'),\n",
    "                end_time=line.get('end_time')\n",
    "            )\n",
    "            session.add(transcript_entry)\n",
    "        session.commit()\n",
    "        print(\"Transcript lines inserted successfully using SQLAlchemy!\")\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"Error inserting transcript lines: {e}\")\n",
    "    finally:\n",
    "        session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript_file(transcript_file):\n",
    "    \"\"\"\n",
    "    Parse the transcript file and create a list of dictionaries for insertion.\n",
    "    Each line in the transcript is assumed to be in the format:\n",
    "      [Role] Speaker Label: Transcript text\n",
    "    The function extracts:\n",
    "      - role: text within the square brackets\n",
    "      - speaker_label: text between the square bracket and the colon\n",
    "      - transcript: text after the colon.\n",
    "    Since no meeting_id, start_time, or end_time information is available, these will be set to None.\n",
    "    \"\"\"\n",
    "    transcript_lines = []\n",
    "    with open(transcript_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Find the role in square brackets\n",
    "            if line.startswith('['):\n",
    "                try:\n",
    "                    role_end = line.index(']')\n",
    "                    role = line[1:role_end]\n",
    "                except ValueError:\n",
    "                    role = \"Unknown\"\n",
    "            else:\n",
    "                role = \"Unknown\"\n",
    "            \n",
    "            # Split at the colon\n",
    "            if ':' in line:\n",
    "                header, transcript_text = line.split(':', 1)\n",
    "            else:\n",
    "                header, transcript_text = line, \"\"\n",
    "            \n",
    "            # Remove the role part from header to get the speaker label\n",
    "            speaker_label = header.replace(f'[{role}]', '').strip()\n",
    "            \n",
    "            transcript_lines.append({\n",
    "                \"meeting_id\": None,\n",
    "                \"speaker_label\": f\"{role} - {speaker_label}\",\n",
    "                \"transcript\": transcript_text.strip(),\n",
    "                \"start_time\": None,\n",
    "                \"end_time\": None\n",
    "            })\n",
    "    return transcript_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Transcript\n",
    "\n",
    "Finally, the processed transcript—with speaker and role labels—is saved to a text file. Each line in the output file is formatted as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, model=\"base\", output=\"role_transcript.txt\", \n",
    "                  min_speakers=2, max_speakers=2,\n",
    "                  db_url=\"postgresql+psycopg2://user:1234@192.168.10.132:5432/amas\"):\n",
    "    # Create temporary directory for audio extraction\n",
    "    os.makedirs(\"temp\", exist_ok=True)\n",
    "    audio_path = os.path.join(\"temp\", os.path.basename(video_path) + \".wav\")\n",
    "    \n",
    "    # Step 1: Extract audio from video\n",
    "    if not convert_video_to_audio(video_path, audio_path):\n",
    "        print(\"Video conversion failed.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Transcribe audio using Whisper\n",
    "    transcription = transcribe_audio(audio_path, model)\n",
    "    if transcription is None:\n",
    "        print(\"Transcription failed.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Load Silero VAD\n",
    "    vad_model, get_speech_timestamps, read_audio = load_silero_vad()\n",
    "    if vad_model is None:\n",
    "        print(\"Silero VAD failed to load. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Get speech embeddings and segments using VAD\n",
    "    embeddings, vad_segments = get_speech_embeddings(audio_path, vad_model, get_speech_timestamps, read_audio)\n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        print(\"No speech segments detected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 5: Cluster segments to assign speaker labels\n",
    "    vad_segments = cluster_speakers(embeddings, vad_segments,\n",
    "                                    min_speakers=min_speakers,\n",
    "                                    max_speakers=max_speakers)\n",
    "    \n",
    "    # Step 6: Assign Whisper transcript segments to speakers based on time overlap\n",
    "    segments_with_speakers = assign_transcript_to_speakers(transcription[\"segments\"], vad_segments)\n",
    "    \n",
    "    # Step 7: Assign roles using a simple heuristic (e.g., based on question counts)\n",
    "    segments_with_roles = assign_roles(segments_with_speakers)\n",
    "    \n",
    "    # Step 8: Save the formatted transcript and retrieve transcript lines\n",
    "    transcript_lines = save_formatted_transcript(segments_with_roles, output)\n",
    "    \n",
    "    # Clean up temporary audio file\n",
    "    os.remove(audio_path)\n",
    "    print(\"\\nProcessing completed successfully!\")\n",
    "    \n",
    "    transcript_lines = parse_transcript_file(output)\n",
    "    \n",
    "    # Step 9: Insert transcript lines into PostgreSQL using SQLAlchemy\n",
    "    insert_transcript_lines_sqlalchemy(db_url, transcript_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted successfully to temp\\sample_audio_for_transcript.mp3.wav\n",
      "Loading Whisper model: base\n",
      "Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Learning\\lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:03.000]  Yes, Gauri, tell me something about you.\n",
      "[00:04.000 --> 00:07.000]  Thank you for allowing me to introduce myself.\n",
      "[00:08.000 --> 00:09.000]  My name is Gauri Swamali.\n",
      "[00:09.000 --> 00:11.000]  I am from Rajasthan.\n",
      "[00:11.000 --> 00:16.000]  I am doing computer science engineering from Githa Anjali Institute of Technical Studies.\n",
      "[00:16.000 --> 00:23.000]  I secured 9.2 CGPA in 10 standard, 73.4 CG percentage and 12 standard.\n",
      "[00:23.000 --> 00:27.000]  And currently in engineering, my aggregates code is 77 percentage.\n",
      "[00:28.000 --> 00:32.000]  Computer science is omnipresent, that is, it is present in every field.\n",
      "[00:32.000 --> 00:39.000]  And hence, I have invested these last 3.5 almost years in developing my software skills.\n",
      "[00:39.000 --> 00:46.000]  My technical skills include the programming in Python, Java, C, C++ databases.\n",
      "[00:46.000 --> 00:53.000]  I am also familiar with web development, Kubernetes, Docker, Ansible, etc.\n",
      "[00:53.000 --> 00:56.000]  So, these are all my technical skills.\n",
      "[00:56.000 --> 01:03.000]  And I have also used these skills to build several projects, which include user management project and attendance management project.\n",
      "[01:03.000 --> 01:11.000]  I have also participated in smart India hackathons, which in one we also got to the finals.\n",
      "[01:11.000 --> 01:14.000]  These are all more my technical skills.\n",
      "[01:14.000 --> 01:20.000]  Coming to my extracurricular activities, I love to dance, particularly semi-classical dance.\n",
      "[01:20.000 --> 01:23.000]  I also like to read, write and paint.\n",
      "[01:23.000 --> 01:27.000]  I have been the student editor of our college's newsletter, Bits and Bites.\n",
      "[01:27.000 --> 01:32.000]  And currently, I am one of the cultural heads of our students club.\n",
      "[01:32.000 --> 01:38.000]  So, I have also participated in several donation camps.\n",
      "[01:38.000 --> 01:44.000]  Several donation camps as I am a member of the Rotary Club of Apple.\n",
      "[01:44.000 --> 01:47.000]  I consider myself as a very focused person.\n",
      "[01:47.000 --> 01:52.000]  And I always work towards my goals in a very efficient manner.\n",
      "[01:52.000 --> 01:56.000]  I am a team player and very optimistic and tough times.\n",
      "[01:56.000 --> 02:04.000]  And lastly, I would like to say that, why not start the journey of my success by the success of joining TCS?\n",
      "[02:04.000 --> 02:06.000]  Thank you so much.\n",
      "[02:07.000 --> 02:08.000]  Okay.\n",
      "[02:08.000 --> 02:19.000]  What are the five specific points about TCS, which makes TCS different from other ITMNCs?\n",
      "[02:19.000 --> 02:25.000]  Firstly, TCS has a brand name.\n",
      "[02:25.000 --> 02:30.000]  It is a multinational company and it has a very good brand name.\n",
      "[02:30.000 --> 02:33.000]  And thus, it would give me a good start for my career.\n",
      "[02:34.000 --> 02:36.000]  Secondly, it has a work-life balance.\n",
      "[02:36.000 --> 02:44.000]  I have talked to many of my friends who are in TCS and they say that TCS gives a very good work-life balance.\n",
      "[02:44.000 --> 02:46.000]  So, that is the second point.\n",
      "[02:46.000 --> 02:51.000]  Third point is the colleagues there are very futuristic in their skills.\n",
      "[02:51.000 --> 02:54.000]  So, there is a lot to learn from them.\n",
      "[02:54.000 --> 02:58.000]  Fourthly, as TCS is a multinational company, I would like to...\n",
      "[02:59.000 --> 03:06.000]  I would have the chance to even go abroad and meet people of different nationalities.\n",
      "[03:06.000 --> 03:13.000]  And lastly, as TCS has been in the industry for almost 150 years,\n",
      "[03:13.000 --> 03:17.000]  so it gives me a sense of job security to some extent.\n",
      "[03:17.000 --> 03:21.000]  Okay. What are the ethical values TCS have?\n",
      "[03:21.000 --> 03:28.000]  So, like I said, there is a work-life balance and it promotes gender equality.\n",
      "[03:28.000 --> 03:33.000]  There is no discrimination on the basis of that.\n",
      "[03:33.000 --> 03:39.000]  And so, it is very good in ethics overall.\n",
      "[03:39.000 --> 03:45.000]  Okay. Okay, Gare, nice talking to you.\n",
      "[03:45.000 --> 03:46.000]  Thank you, sir.\n",
      "[03:46.000 --> 03:47.000]  Karna Radford, sir.\n",
      "[03:47.000 --> 03:49.000]  So, take next part of the interview.\n",
      "[03:49.000 --> 03:50.000]  Thank you, sir.\n",
      "[03:50.000 --> 03:51.000]  Good afternoon, sir.\n",
      "[03:51.000 --> 03:52.000]  All prepared?\n",
      "[03:52.000 --> 03:53.000]  Yes, sir.\n",
      "[03:53.000 --> 03:54.000]  Okay.\n",
      "[03:54.000 --> 04:05.000]  Tell us which other campus interviews have you appeared for and what has been the results?\n",
      "[04:05.000 --> 04:10.000]  So, I have interviewed for LANET, which is, you know, DAPUR only.\n",
      "[04:10.000 --> 04:12.000]  And I got selected for it.\n",
      "[04:12.000 --> 04:16.000]  I also interviewed for Gateway Industries.\n",
      "[04:16.000 --> 04:18.000]  I was not selected in the interview.\n",
      "[04:18.000 --> 04:21.000]  I got to the interview part but was not selected.\n",
      "[04:21.000 --> 04:22.000]  So, that's it.\n",
      "[04:22.000 --> 04:24.000]  I've only given these interviews.\n",
      "[04:24.000 --> 04:25.000]  Okay.\n",
      "[04:25.000 --> 04:33.000]  Have you analyzed why you didn't get selected for Gateway?\n",
      "[04:33.000 --> 04:42.000]  So, in Gateway, I might have not answered the technical questions very well as compared to the other candidates.\n",
      "[04:42.000 --> 04:43.000]  Okay.\n",
      "[04:43.000 --> 04:47.000]  So, how has you prepared for a current TCS interview now?\n",
      "[04:47.000 --> 04:48.000]  Okay.\n",
      "[04:48.000 --> 04:51.000]  So, I have enhanced my technical skills.\n",
      "[04:51.000 --> 04:57.000]  I have been preparing for the core subjects and I've been doing coding almost every day.\n",
      "[04:57.000 --> 05:01.000]  So, I think that I've prepared quite well for TCS.\n",
      "[05:01.000 --> 05:02.000]  Okay.\n",
      "[05:02.000 --> 05:05.000]  What do you tell me?\n",
      "[05:05.000 --> 05:08.000]  What are your future plans?\n",
      "[05:08.000 --> 05:10.000]  So, my future plans.\n",
      "[05:10.000 --> 05:14.000]  So, if I join TCS or otherwise?\n",
      "[05:14.000 --> 05:15.000]  What are your future plans?\n",
      "[05:15.000 --> 05:20.000]  So, I want to join a software company and work for it.\n",
      "[05:20.000 --> 05:29.000]  And I want to learn as much languages and as much skills as I can.\n",
      "[05:29.000 --> 05:35.000]  And I also want to achieve great success so that I'm satisfied with what I've learned.\n",
      "[05:35.000 --> 05:38.000]  Okay.\n",
      "[05:38.000 --> 05:44.000]  What do you think other reasons for youngsters switching over jobs?\n",
      "[05:45.000 --> 05:53.000]  So, maybe it is a job satisfaction because they enter a company without seeing the background of the company\n",
      "[05:53.000 --> 05:59.000]  and then they later find out that the company was not suitable for them.\n",
      "[05:59.000 --> 06:01.000]  Okay.\n",
      "[06:01.000 --> 06:13.000]  As for you, what are the three important things that one should consider before joining any organization as a fresher?\n",
      "[06:13.000 --> 06:19.000]  So, firstly, the one should look if there is a lot to learn from the company.\n",
      "[06:19.000 --> 06:25.000]  I mean, there should be a very steep growing curve in the company.\n",
      "[06:25.000 --> 06:30.000]  Secondly, there should be colleagues and everything.\n",
      "[06:30.000 --> 06:35.000]  It should be very frank so that they help you in every way.\n",
      "[06:35.000 --> 06:40.000]  And thirdly, for me, there should be a brand name so that there is a satisfaction that\n",
      "[06:40.000 --> 06:45.000]  yes, I've achieved a level and now I can go to the next level.\n",
      "[06:45.000 --> 06:46.000]  Okay.\n",
      "[06:46.000 --> 06:50.000]  Technically, what gives you job satisfaction?\n",
      "[06:50.000 --> 06:56.000]  So, technically, the latest, I want to work with latest technical skills,\n",
      "[06:56.000 --> 07:01.000]  like not obsolete technologies, but the latest technologies.\n",
      "[07:01.000 --> 07:06.000]  So, how does DCS fit into your requirement?\n",
      "[07:06.000 --> 07:11.000]  DCS changes its technologies according to the needs of the people of the earth.\n",
      "[07:11.000 --> 07:17.000]  So, there is no fixed technology that they use.\n",
      "[07:17.000 --> 07:20.000]  They use according to their clients, which is very well.\n",
      "[07:20.000 --> 07:23.000]  Which are the major clients of DCS?\n",
      "[07:23.000 --> 07:26.000]  Can you name three or four of them?\n",
      "[07:26.000 --> 07:31.000]  I'm sorry, but I'm not aware of the clients.\n",
      "[07:31.000 --> 07:33.000]  Okay.\n",
      "[07:34.000 --> 07:39.000]  Anything that you want to ask?\n",
      "[07:39.000 --> 07:45.000]  So, will I be able to like fit in the company?\n",
      "[07:45.000 --> 07:49.000]  I mean, what do you think?\n",
      "[07:49.000 --> 07:52.000]  Will you be able to fit into the company?\n",
      "[07:52.000 --> 07:53.000]  Yes, sir.\n",
      "[07:53.000 --> 07:54.000]  Yeah.\n",
      "[07:54.000 --> 08:01.000]  As for your own analysis, you said you're a good team player and what requirements you said for a good job?\n",
      "[08:01.000 --> 08:02.000]  Yes, sir.\n",
      "[08:02.000 --> 08:05.000]  In case you have those qualities, I'm sure you'll be able to fit into.\n",
      "[08:05.000 --> 08:06.000]  Okay, sir.\n",
      "[08:06.000 --> 08:07.000]  Thank you.\n",
      "[08:07.000 --> 08:08.000]  Any more suggestions, Sony?\n",
      "[08:08.000 --> 08:09.000]  Yes.\n",
      "[08:09.000 --> 08:11.000]  No, nothing more.\n",
      "[08:11.000 --> 08:14.000]  Carvin sir will finally give his remarks, right?\n",
      "[08:14.000 --> 08:15.000]  Okay.\n",
      "[08:15.000 --> 08:16.000]  Thank you so much, sir.\n",
      "[08:16.000 --> 08:17.000]  All the best.\n",
      "[08:17.000 --> 08:18.000]  Right?\n",
      "[08:18.000 --> 08:19.000]  Thank you, sir.\n",
      "[08:19.000 --> 08:20.000]  Gauri?\n",
      "[08:20.000 --> 08:21.000]  Yes, sir.\n",
      "[08:21.000 --> 08:23.000]  What is your, basically, the aim?\n",
      "[08:23.000 --> 08:27.000]  You want to go in the software companies or anything more than that?\n",
      "[08:27.000 --> 08:28.000]  No, sir.\n",
      "[08:28.000 --> 08:29.000]  I want to go in the software companies.\n",
      "[08:29.000 --> 08:30.000]  That is only me.\n",
      "[08:30.000 --> 08:31.000]  Okay.\n",
      "[08:31.000 --> 08:37.000]  If you don't want to go for any higher studies or entrepreneurship like?\n",
      "[08:37.000 --> 08:44.000]  No, so entrepreneurship, no, but higher studies, many of the companies, TCS also provides,\n",
      "[08:44.000 --> 08:47.000]  like, simultaneously, higher studies.\n",
      "[08:47.000 --> 08:50.000]  So that may also be a very good option.\n",
      "[08:50.000 --> 08:51.000]  Okay.\n",
      "[08:51.000 --> 08:57.000]  For example, you have got the offer from TCS.\n",
      "[08:57.000 --> 09:05.000]  At the same time, meanwhile, getting the better offer than TCS from another organization,\n",
      "[09:05.000 --> 09:08.000]  which is not a tier one organization, but tier two organization.\n",
      "[09:08.000 --> 09:11.000]  But the package is just double of what we are offering.\n",
      "[09:11.000 --> 09:14.000]  Then what do you do?\n",
      "[09:14.000 --> 09:19.000]  So I compare both the companies in terms of the...\n",
      "[09:19.000 --> 09:24.000]  So, we definitely have told that TCS is a tier one and you have got the offer from tier two.\n",
      "[09:24.000 --> 09:27.000]  It means you start up a mid-sized company.\n",
      "[09:27.000 --> 09:30.000]  With the package, for example, we are giving you a product.\n",
      "[09:30.000 --> 09:32.000]  They are giving you 8 lakhs.\n",
      "[09:32.000 --> 09:34.000]  In that case, what will you do?\n",
      "[09:34.000 --> 09:42.000]  So I choose TCS only because TCS is like almost 150 years old company.\n",
      "[09:42.000 --> 09:47.000]  So it has a very good job security.\n",
      "[09:47.000 --> 09:55.000]  And as I cannot really trust a startup because they are new in the industry.\n",
      "[09:55.000 --> 09:58.000]  And that's why there might be some trust issues.\n",
      "[09:58.000 --> 10:01.000]  So I guess that it does not matter for you.\n",
      "[10:01.000 --> 10:05.000]  So job security is more important than salary.\n",
      "[10:05.000 --> 10:11.000]  If you do not perform well in the TCS, also you can get the job insecurity in TCS also.\n",
      "[10:11.000 --> 10:14.000]  But I will perform good.\n",
      "[10:14.000 --> 10:18.000]  Like from May and there will be no complaints from TCS.\n",
      "[10:18.000 --> 10:20.000]  Okay.\n",
      "[10:20.000 --> 10:23.000]  Yeah.\n",
      "[10:23.000 --> 10:26.000]  Good afternoon, sir.\n",
      "[10:26.000 --> 10:28.000]  Good afternoon.\n",
      "[10:28.000 --> 10:32.000]  Which domain do you feel comfortable?\n",
      "[10:32.000 --> 10:35.000]  So course objects programming.\n",
      "[10:35.000 --> 10:38.000]  C++, TPSN.\n",
      "[10:39.000 --> 10:44.000]  So give me a concept to implement SNAC and LIDER.\n",
      "[10:44.000 --> 10:48.000]  To implement SNACs and DADDERS.\n",
      "[10:48.000 --> 10:50.000]  Using any programming language.\n",
      "[10:50.000 --> 10:53.000]  I want to know only concept, not the implementation.\n",
      "[10:53.000 --> 10:55.000]  Okay.\n",
      "[10:55.000 --> 10:58.000]  I'm giving you the one minute.\n",
      "[10:58.000 --> 11:00.000]  So can I write something?\n",
      "[11:00.000 --> 11:02.000]  I just need to know.\n",
      "[11:02.000 --> 11:05.000]  But I need only logic, not the coding or port.\n",
      "[11:05.000 --> 11:08.000]  Okay, sir.\n",
      "[11:35.000 --> 11:38.000]  Thank you.\n",
      "[12:35.000 --> 12:42.000]  Yes sir, yes sir.\n",
      "[12:42.000 --> 12:46.160]  So, we can, where really, but then I can analyze that you are going in the right direction\n",
      "[12:46.160 --> 12:47.160]  on.\n",
      "[12:47.160 --> 12:48.160]  Okay, sir.\n",
      "[12:48.160 --> 12:59.200]  So, firstly, we will, we will like a list, for example, of how many columns are there.\n",
      "[12:59.200 --> 13:07.520]  For example, I take a list of 50, the maximum we go to 50 and then now we take a dice variable\n",
      "[13:07.520 --> 13:12.440]  in which we will take a random function with the help of random function.\n",
      "[13:12.440 --> 13:20.240]  We can derive a number between 1 to 6 and for the first, for the first move, if the number\n",
      "[13:20.240 --> 13:26.240]  random number is 6, then we can move like we can keep a counter of the player.\n",
      "[13:26.400 --> 13:32.880]  So, we can increase the counter with the number that comes in the dice and then we can\n",
      "[13:32.880 --> 13:41.760]  pre-define the snakes, for example, if at, if we reach 5, then the counter should\n",
      "[13:41.760 --> 13:45.360]  like increase by 10 blocks.\n",
      "[13:45.360 --> 13:52.080]  So, the counter will increase by 10 blocks, that is a ladder and this way we can like pre-define\n",
      "[13:52.080 --> 13:59.120]  the next question is then how you will determine whether to, you have to increment its dice value,\n",
      "[13:59.120 --> 14:04.480]  means you, you want to use its increasing the counter value, inform a platter or you have to\n",
      "[14:04.480 --> 14:06.640]  decrement the counter value in form or snake.\n",
      "[14:06.640 --> 14:08.400]  So, how you will determine it?\n",
      "[14:08.400 --> 14:14.960]  So, with the help of functions, for example, of the dice value, for if, if the dice value or\n",
      "[14:15.440 --> 14:20.640]  that block value in which the counter is right now is 5.\n",
      "[14:20.640 --> 14:25.520]  So, I have assigned 5 already that 5 is a ladder.\n",
      "[14:25.520 --> 14:32.080]  So, then it will increase and for example, if 12 is a snake, then I have already defined that 12,\n",
      "[14:32.080 --> 14:36.400]  at 12, you need to decrement the value by, for example, 5 or whatever.\n",
      "[14:36.400 --> 14:40.080]  So, I will pre-define, where you will pre-define these values?\n",
      "[14:40.800 --> 14:46.160]  So, I can use a dictionary to like if I am doing in Python, then I can do some things.\n",
      "[14:46.160 --> 14:51.360]  And then if we are using C, then then CC, then there if it does not have any dictionary.\n",
      "[14:54.320 --> 15:05.360]  Then so, we can, we can use an array of arrays and then like make a dictionary with the help of arrays.\n",
      "[15:05.840 --> 15:08.160]  Okay. Yes sir.\n",
      "[15:08.160 --> 15:10.160]  Can we use switch or not there?\n",
      "[15:12.560 --> 15:22.720]  Switch, yes. But so, we can use it. But it is too long, like we will have to write the code many times\n",
      "[15:23.440 --> 15:30.480]  in switch. One many times we can define a switch in a global or anywhere, then we can use it.\n",
      "[15:31.120 --> 15:35.840]  Okay. So, switch for us, snake and switch for a ladder, two separate switches we can use.\n",
      "[15:35.840 --> 15:37.040]  Yes sir, that we can.\n",
      "[15:38.800 --> 15:42.800]  Okay. So, what do mean by the encapsulation? Is the Vianchancer is correct?\n",
      "[15:44.160 --> 15:49.600]  So, partially it is correct. It is used for data iding but more data security.\n",
      "[15:49.600 --> 15:57.520]  So, in a class it is implemented using access modifiers like public protected private and there\n",
      "[15:57.520 --> 16:06.080]  is a default access specifiers as well. So, using these we can define the scope of the variables\n",
      "[16:06.720 --> 16:14.800]  or the functions like if it is public then it can be used all through and if it is protected then only\n",
      "[16:14.800 --> 16:20.640]  the subclasses can use it and if private then only the current class can use it.\n",
      "[16:21.600 --> 16:24.080]  And what do you mean by storage classes?\n",
      "[16:26.720 --> 16:29.920]  Storage classes. So, I am not able to recall. I\n",
      "[16:34.000 --> 16:35.920]  Okay. What are you all the best?\n",
      "[16:36.640 --> 16:37.760]  Okay. Thank you so much.\n",
      "[16:37.760 --> 16:42.000]  It is your more and more TCS technical integral questions, right?\n",
      "[16:42.000 --> 16:44.240]  They are basically mainly from CNC plus bus.\n",
      "[16:45.200 --> 16:48.320]  Okay, sir. I will keep that in mind. Thank you so much.\n",
      "[16:48.320 --> 16:49.120]  All the best.\n",
      "[16:49.120 --> 16:49.760]  Thank you, sir.\n",
      "[16:51.040 --> 16:56.880]  From my side and the networks are into you very much for you, definitely you are the good candidate\n",
      "[16:56.880 --> 16:59.120]  that we have already observed many times.\n",
      "[17:01.360 --> 17:06.000]  Technical knowledge I have not checked, do not know if your head-out department CSC will check it out.\n",
      "[17:06.880 --> 17:11.280]  But you can easily correct the process.\n",
      "[17:11.920 --> 17:13.200]  Okay. Thank you so much, sir.\n",
      "[17:13.520 --> 17:20.000]  Or when you are clear about your thought process then at least it should be justified\n",
      "[17:20.000 --> 17:22.720]  when you are giving the interest. Yes, sir.\n",
      "[17:24.320 --> 17:29.840]  That is very important. They will feel that you want to come but you cannot come.\n",
      "[17:31.040 --> 17:36.800]  Then there is a chance they can drop you either you are a good candidate or the best.\n",
      "[17:36.800 --> 17:42.320]  For example, Google want to hire somebody or they find the candidate which is having the\n",
      "[17:42.320 --> 17:49.920]  capabilities to get a corrode package in an upright. But they have the doubt that no doubt is the\n",
      "[17:49.920 --> 17:55.040]  best but they have the doubt that either he will join or not then might be they can drop.\n",
      "[17:55.920 --> 18:00.480]  So, in the interview at least it should not be so case.\n",
      "[18:01.360 --> 18:03.600]  Okay, I will keep that in mind. So, thank you.\n",
      "[18:03.600 --> 18:04.560]  You are also at right.\n",
      "[18:05.120 --> 18:05.600]  Okay, sir.\n",
      "[18:06.720 --> 18:07.200]  Thank you.\n",
      "[18:07.600 --> 18:08.160]  Thank you, sir.\n",
      "Loading Silero VAD model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\HARSH DADIYA/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 141 speech segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARSH DADIYA\\AppData\\Local\\Temp\\ipykernel_9912\\2128570759.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segment_tensor = torch.tensor(speech_segment).clone().detach().float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: 2, Silhouette Score: 0.305\n",
      "Selected 2 speakers with silhouette score: 0.305\n",
      "Role-based transcript saved to role_transcript.txt\n",
      "\n",
      "Processing completed successfully!\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) connection to server at \"192.168.10.132\", port 5432 failed: Connection timed out (0x0000274C/10060)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3298\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3277\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m \n\u001b[0;32m   3279\u001b[0m \u001b[38;5;124;03mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \n\u001b[0;32m   3297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:1263\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1263\u001b[0m     fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:712\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dec_overflow()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:674\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    901\u001b[0m         pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on connect(): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:896\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 896\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[1;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\default.py:622\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams):\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m _connect(dsn, connection_factory\u001b[38;5;241m=\u001b[39mconnection_factory, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOperationalError\u001b[0m: connection to server at \"192.168.10.132\", port 5432 failed: Connection timed out (0x0000274C/10060)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHARSH DADIYA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msample_audio_for_transcript.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 52\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, model, output, min_speakers, max_speakers, db_url)\u001b[0m\n\u001b[0;32m     49\u001b[0m transcript_lines \u001b[38;5;241m=\u001b[39m parse_transcript_file(output)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Step 9: Insert transcript lines into PostgreSQL using SQLAlchemy\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43minsert_transcript_lines_sqlalchemy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscript_lines\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 23\u001b[0m, in \u001b[0;36minsert_transcript_lines_sqlalchemy\u001b[1;34m(db_url, transcript_lines)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mInsert transcript lines into the PostgreSQL database using SQLAlchemy.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m  - end_time (float)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(db_url)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Create table if it doesn't exist.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m Session \u001b[38;5;241m=\u001b[39m sessionmaker(bind\u001b[38;5;241m=\u001b[39mengine)\n\u001b[0;32m     25\u001b[0m session \u001b[38;5;241m=\u001b[39m Session()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\sql\\schema.py:5900\u001b[0m, in \u001b[0;36mMetaData.create_all\u001b[1;34m(self, bind, tables, checkfirst)\u001b[0m\n\u001b[0;32m   5876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_all\u001b[39m(\n\u001b[0;32m   5877\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5878\u001b[0m     bind: _CreateDropBind,\n\u001b[0;32m   5879\u001b[0m     tables: Optional[_typing_Sequence[Table]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5880\u001b[0m     checkfirst: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   5881\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5882\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create all tables stored in this metadata.\u001b[39;00m\n\u001b[0;32m   5883\u001b[0m \n\u001b[0;32m   5884\u001b[0m \u001b[38;5;124;03m    Conditional by default, will not attempt to recreate tables already\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5898\u001b[0m \n\u001b[0;32m   5899\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5900\u001b[0m     \u001b[43mbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ddl_visitor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mddl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSchemaGenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtables\u001b[49m\n\u001b[0;32m   5902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3248\u001b[0m, in \u001b[0;36mEngine._run_ddl_visitor\u001b[1;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[0;32m   3242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_ddl_visitor\u001b[39m(\n\u001b[0;32m   3243\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3244\u001b[0m     visitorcallable: Type[Union[SchemaGenerator, SchemaDropper]],\n\u001b[0;32m   3245\u001b[0m     element: SchemaItem,\n\u001b[0;32m   3246\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   3247\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m   3249\u001b[0m         conn\u001b[38;5;241m.\u001b[39m_run_ddl_visitor(visitorcallable, element, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3238\u001b[0m, in \u001b[0;36mEngine.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m   3214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbegin\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Connection]:\n\u001b[0;32m   3215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a context manager delivering a :class:`_engine.Connection`\u001b[39;00m\n\u001b[0;32m   3216\u001b[0m \u001b[38;5;124;03m    with a :class:`.Transaction` established.\u001b[39;00m\n\u001b[0;32m   3217\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3236\u001b[0m \n\u001b[0;32m   3237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m-> 3238\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m   3239\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mbegin():\n\u001b[0;32m   3240\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m conn\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3274\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Connection:\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3253\u001b[0m \n\u001b[0;32m   3254\u001b[0m \u001b[38;5;124;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3271\u001b[0m \n\u001b[0;32m   3272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:148\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mraw_connection()\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 148\u001b[0m         \u001b[43mConnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception_noconnection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2439\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[1;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[0;32m   2437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2441\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:146\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    148\u001b[0m         Connection\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[0;32m    149\u001b[0m             err, dialect, engine\n\u001b[0;32m    150\u001b[0m         )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3298\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m   3277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m \n\u001b[0;32m   3279\u001b[0m \u001b[38;5;124;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3296\u001b[0m \n\u001b[0;32m   3297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:449\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PoolProxiedConnection:\n\u001b[0;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionFairy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:1263\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_checkout\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     fairy: Optional[_ConnectionFairy] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ConnectionFairy:\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m-> 1263\u001b[0m         fairy \u001b[38;5;241m=\u001b[39m \u001b[43m_ConnectionRecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m threadconns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             threadconns\u001b[38;5;241m.\u001b[39mcurrent \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(fairy)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:712\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    710\u001b[0m     rec \u001b[38;5;241m=\u001b[39m cast(_ConnectionRecord, pool\u001b[38;5;241m.\u001b[39m_do_get())\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     dbapi_connection \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mget_connection()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\impl.py:179\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection()\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dec_overflow()\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inc_overflow():\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:390\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConnectionPoolEntry:\n\u001b[0;32m    388\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConnectionRecord\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:674\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pool \u001b[38;5;241m=\u001b[39m pool\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:900\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 900\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    901\u001b[0m         pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on connect(): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\pool\\base.py:896\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m     pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\create.py:646\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[1;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\sqlalchemy\\engine\\default.py:622\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams):\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_dbapi\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Learning\\lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m _connect(dsn, connection_factory\u001b[38;5;241m=\u001b[39mconnection_factory, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: (psycopg2.OperationalError) connection to server at \"192.168.10.132\", port 5432 failed: Connection timed out (0x0000274C/10060)\n\tIs the server running on that host and accepting TCP/IP connections?\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "process_video(r\"C:\\Users\\HARSH DADIYA\\Downloads\\sample_audio_for_transcript.mp3\", model=\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
